# -*- coding: utf-8 -*-
"""Another copy of Master_set_up

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15bQOTz-O-8Wf36TCmErdqLowYDSaZyFM
"""

!pip install biopython

from Bio import Entrez
from Bio import SeqIO
print("Biopython installed successfully!")

"""Helper Functions to help us find the locations of sequences

**Stage 1 & 2: Data Collection & Data Processing**

In general we shouldn't download files locally because when we go to submit the grader will also have to all the same uploading/downloading that we as the developer do. Instead we should save this to a variable and then at the end we can print the results in something like seaborn, or some other data visualization library
"""

from typing import List, Dict, TypedDict
from Bio import Entrez, SeqIO
from Bio.SeqUtils import gc_fraction
import pandas as pd


# A typed dictionary for sequence data
class SequenceData(TypedDict):
    ID: str
    Description: str
    Sequence: str
    Length: int


def fetch_sequences_to_dataframe(query: str, email: str, retmax:int=3) -> pd.DataFrame:
    """Fetch sequences from NCBI based on a query and return as a DataFrame.

    Args:
        query (str): Search term for NCBI nucleotide database.
        email (str): Email address to identify the user in Entrez.

    Returns:
        pd.DataFrame: DataFrame with sequence data, including ID, description, sequence, length, and GC content.
    """
    Entrez.email = email

    # Search NCBI nucleotide database
    with Entrez.esearch(db="nucleotide", term=query, retmax=retmax) as search_handle:
        search_results = Entrez.read(search_handle)
        ids: List[str] = search_results["IdList"]

    if not ids:
        print(f"No results found for query: {query}")
        return pd.DataFrame()

    # Fetch sequences for the found IDs
    sequences: List[SequenceData] = []
    with Entrez.efetch(db="nucleotide", id=",".join(ids), rettype="fasta", retmode="text") as fetch_handle:
        for record in SeqIO.parse(fetch_handle, "fasta"):
            seq = str(record.seq)
            sequences.append({
                "ID": record.id,
                "Sequence": seq,
                "Length": len(seq),
                "GC_Content": (seq.count('G') + seq.count('C')) / len(seq) * 100
              })

    # Convert to DataFrame
    df = pd.DataFrame(sequences)
    return df

query: str = "blastn"
email: str = "madelyn.mills.112@my.csun.edu"
df: pd.DataFrame = fetch_sequences_to_dataframe(query, email, 5)

# Display the DataFrame
print(df.head())

"""**Stage 2: Data Processing**"""

import re
def find_sequence_positions(sequence: str, pattern: str) -> List[int]:
    """Generic function to find all positions of a pattern in a sequence.

    Args:
        sequence (str): The DNA/RNA sequence to search in
        pattern (str): The pattern to search for

    Returns:
        List[int]: List of starting positions where the pattern was found
    """
    positions = [idx for idx in range(len(sequence) - len(pattern) + 1)
                if sequence[idx:idx + len(pattern)] == pattern]
    return positions

def find_poly_a_tails(sequence: str) -> List[int]:
    """Find poly-A tails (150-250 consecutive A's) in a sequence.

    Args:
        sequence (str): The DNA/RNA sequence to search in

    Returns:
        List[int]: List of starting positions of poly-A tails
    """
    poly_a_pattern = r"A{150,250}"
    matches = list(re.finditer(poly_a_pattern, sequence))
    return [match.start() for match in matches]

def find_u_or_t_positions(sequence: str) -> List[int]:
    """Find positions of both UUUU and TTTT in a sequence.

    Args:
        sequence (str): The DNA/RNA sequence to search in

    Returns:
        List[int]: Combined list of starting positions for UUUU and TTTT
    """
    uuuu_positions = find_sequence_positions(sequence, 'UUUU')
    tttt_positions = find_sequence_positions(sequence, 'TTTT')
    # Combine and sort all positions
    return sorted(uuuu_positions + tttt_positions)

df['UUUU_or_TTTT_positions'] = df['Sequence'].apply(find_u_or_t_positions)

df['UUGCU_positions'] = df['Sequence'].apply(lambda x: find_sequence_positions(x, 'UUGCU'))

df['polyA_positions'] = df['Sequence'].apply(find_poly_a_tails)

df['GGAG_positions'] = df['Sequence'].apply(lambda x:find_sequence_positions(x, 'GGAG'))

print(df.head)

# Here's how you print the number of sequence locations
#printing the last row(4th row)
GGAG_positions = len((df.iloc[-1])['GGAG_positions'])
print(f'GGAG_positons: {GGAG_positions}')

UUUU_or_TTTT_positions = len((df.iloc[-1])['UUUU_or_TTTT_positions'])
print(f'UUUU_or_TTTT_positions: {UUUU_or_TTTT_positions}')

polyA_positions = len((df.iloc[-1])['polyA_positions'])
print(f'polyA_positions: {polyA_positions}')

UUGCU_positions = len((df.iloc[-1])['UUGCU_positions'])
print(f'UUGCU_positions: {UUGCU_positions}')

#how to loop over the rows and print the length of how many times the sequences show up

for index, row in df.iterrows():
  GGAG_positions = len((row['GGAG_positions']))
  UUUU_or_TTTT_positions = len((row['UUUU_or_TTTT_positions']))
  polyA_positions = len((row['polyA_positions']))
  UUGCU_positions = len((row['UUGCU_positions']))
  print(f'row {index+1}', f'GGAG: {GGAG_positions}', f'UUUU/TTTT: {UUUU_or_TTTT_positions}', f'polyA: {polyA_positions}', f'UUGCU: {UUGCU_positions}')

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas numpy scikit-learn seaborn matplotlib

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import seaborn as sns
import matplotlib.pyplot as plt
from typing import Tuple, List, Dict
import re

def visualize_results(results: Dict):
    """Create visualizations for model performance and feature importance.

    Args:
        results (Dict): Dictionary containing model results
    """
    # Use a more reliable style
    plt.style.use('default')

    # Create subplot grid with better spacing
    fig = plt.figure(figsize=(15, 12))
    gs = plt.GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)

    # 1. Model Performance Comparison
    ax1 = fig.add_subplot(gs[0, 0])
    model_names = list(results.keys())
    test_scores = [results[model]['test_score'] for model in model_names]

    bars = ax1.bar(model_names, test_scores, color='skyblue')
    ax1.set_title('Model Performance Comparison', pad=20)
    ax1.set_ylabel('Test Accuracy')
    ax1.set_ylim([0, 1.0])  # Set y-axis from 0 to 1 for accuracy
    # Add value labels on top of bars
    for bar in bars:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom')
    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')

    # 2. Confusion Matrix for Random Forest
    ax2 = fig.add_subplot(gs[0, 1])
    if 'Random Forest' in results:
        rf_results = results['Random Forest']
        cm = confusion_matrix(rf_results['y_test'], rf_results['predictions'])
        sns.heatmap(cm, annot=True, fmt='d', ax=ax2, cmap='Blues',
                   cbar_kws={'label': 'Count'})
        ax2.set_title('Random Forest Confusion Matrix', pad=20)
        ax2.set_xlabel('Predicted Label')
        ax2.set_ylabel('True Label')

    # 3. Cross-validation Scores
    ax3 = fig.add_subplot(gs[1, 0])
    cv_scores = [results[model]['cv_scores'] for model in model_names]
    box_plot = ax3.boxplot(cv_scores, labels=model_names,
                          patch_artist=True)

    # Customize box plot colors
    for box in box_plot['boxes']:
        box.set(facecolor='lightblue', alpha=0.7)
    ax3.set_title('Cross-validation Scores', pad=20)
    ax3.set_ylabel('Accuracy')
    ax3.set_ylim([0, 1.0])  # Set y-axis from 0 to 1 for accuracy
    plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45, ha='right')

    # 4. Feature Importance (for Random Forest)
    ax4 = fig.add_subplot(gs[1, 1])
    if 'Random Forest' in results:
        rf_model = results['Random Forest']['model']
        feature_importance = pd.DataFrame({
            'feature': ['length', 'gc_content', 'uuuu_count', 'uugcu_count',
                       'ggag_count', 'poly_a_count', 'has_poly_a',
                       'uuuu_position_mean', 'ggag_position_mean'],
            'importance': rf_model.feature_importances_
        })
        feature_importance = feature_importance.sort_values('importance', ascending=True)

        bars = ax4.barh(feature_importance['feature'],
                       feature_importance['importance'],
                       color='lightblue')
        ax4.set_title('Feature Importance (Random Forest)', pad=20)
        ax4.set_xlabel('Importance')

        # Add value labels on bars
        for bar in bars:
            width = bar.get_width()
            ax4.text(width, bar.get_y() + bar.get_height()/2.,
                    f'{width:.3f}',
                    ha='left', va='center')

    plt.tight_layout()

    # Add a title to the entire figure
    fig.suptitle('Machine Learning Model Analysis Results',
                fontsize=16, y=1.02)

    return fig


def safe_mean(x) -> float:
    """Safely calculate mean of a list or return -1 if invalid.

    Args:
        x: Input value (list, float, or None)

    Returns:
        float: Mean value or -1 if invalid
    """
    if isinstance(x, list) and len(x) > 0:
        return np.mean(x)
    return -1

def safe_len(x) -> int:
    """Safely calculate length of a list or return 0 if invalid.

    Args:
        x: Input value (list, float, or None)

    Returns:
        int: Length of list or 0 if invalid
    """
    if isinstance(x, list):
        return len(x)
    return 0


    def extract_sequence_features(df: pd.DataFrame) -> pd.DataFrame:
    """Extract relevant features from RNA sequences for ML analysis.

    Args:
        df (pd.DataFrame): Input DataFrame with sequence data

    Returns:
        pd.DataFrame: DataFrame with extracted features
    """
    features = pd.DataFrame()

    df = df.dropna(subset=['Sequence', 'GC_Content'])
    # Basic sequence properties
    features['length'] = df['Sequence'].str.len()
    features['gc_content'] = df['GC_Content']

    # Pattern counts
    patterns = {
        'uuuu_count': r'(UUUU|TTTT)',
        'uugcu_count': 'UUGCU',
        'ggag_count': 'GGAG',
        'poly_a_count': r'A{150,250}'
    }

    for name, pattern in patterns.items():
        features[name] = df['Sequence'].apply(lambda x: len(re.findall(pattern, str(x))))

    # Position-based features with safe handling
    features['has_poly_a'] = df['polyA_positions'].apply(safe_len).astype(bool)
    features['uuuu_position_mean'] = df['UUUU_positions'].apply(safe_mean)
    features['ggag_position_mean'] = df['GGAG_positions'].apply(safe_mean)

    # Print feature statistics to verify data
    print("Feature statistics:")
    print(features.describe())

    return features

def prepare_data(features: pd.DataFrame, threshold: float = 0.7) -> Tuple[np.ndarray, np.ndarray]:
    """Prepare features and create labels based on stability threshold.

    Args:
        features (pd.DataFrame): Feature DataFrame
        threshold (float): Stability threshold for classification

    Returns:
        Tuple[np.ndarray, np.ndarray]: Prepared features and labels
    """
    # Handle missing values
    features = features.fillna(-1)

    # Scale features
    scaler = StandardScaler()
    X = scaler.fit_transform(features)

    # Create labels (example: using GC content as stability indicator)
    y = (features['gc_content'] > threshold).astype(int)

    # Print data shapes
    print(f"\nFeature matrix shape: {X.shape}")
    print(f"Label vector shape: {y.shape}")
    print(f"Class distribution:\n{np.bincount(y)}")

    return X, y

def train_models(X: np.ndarray, y: np.ndarray) -> Dict:
    """Train multiple models and return their performance metrics.

    Args:
        X (np.ndarray): Feature matrix
        y (np.ndarray): Labels

    Returns:
        Dict: Dictionary containing trained models and their metrics
    """
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    # Initialize models
    models = {
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
        # 'SVM': SVC(kernel='rbf', random_state=42),
        # 'Neural Network': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)
    }

    results = {}

    for name, model in models.items():
        print(f"\nTraining {name}...")
        # Train model
        model.fit(X_train, y_train)

        # Calculate metrics
        train_score = model.score(X_train, y_train)
        test_score = model.score(X_test, y_test)
        cv_scores = cross_val_score(model, X, y, cv=5)

        print(f"{name} - Train score: {train_score:.3f}, Test score: {test_score:.3f}")

        # Store results
        results[name] = {
            'model': model,
            'train_score': train_score,
            'test_score': test_score,
            'cv_scores': cv_scores,
            'predictions': model.predict(X_test),
            'y_test': y_test
        }

    return results

# Main execution pipeline
try:
    # 1. Extract features
    print("Extracting features...")
    features = extract_sequence_features(df)

    # 2. Prepare data
    print("\nPreparing data...")
    X, y = prepare_data(features)

    # 3. Train models and get results
    print("\nTraining models...")
    results = train_models(X, y)

    # 4. Visualize results
    print("\nCreating visualizations...")
    visualize_results(results)

    # Print detailed classification reports
    for name, result in results.items():
        print(f"\nClassification Report for {name}:")
        print(classification_report(result['y_test'], result['predictions']))

except Exception as e:
    print(f"An error occurred: {str(e)}")
    print("\nDataFrame information:")
    print(df.info())
    print("\nSample of the DataFrame:")
    print(df.head())

"""**Stage 3: Machine Learning Modeling**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import joblib
import pandas as pd
from google.colab import files

#Load preprocessed data from CSV

#For demonstration, create a mock label column (e.g., a binary label)
df['label'] = (df['GC_Content'] > 50).astype(int)

#Define features (X) and target (Y)
X = df[['Length', 'GC_Content']]
Y = df['label']

#Split data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

#Train the Random Forest Model
model = RandomForestClassifier()
model.fit(X_train, Y_train)

#Evaluate the Model
Y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(Y_test, Y_pred):.2f}")

#Save the model to a file
joblib.dump(model, "rna_model.joblib")
files.download("rna_model.joblib")

"""**Stage 4: Visualization**"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from google.colab import files

#Loading preprocessed data
# df = pd.read_csv("preprocessed_rna_data.csv")

#Plot GC content distribution and save the plot
plt.figure(figsize=(8, 6))
sns.histplot(df['GC_Content'], kde=True)
plt.xlabel('GC Content (%)')
plt.ylabel('Frequency')
plt.title('Distribution of GC Content in RNA Sequences')
# plt.savefig("gc_content_distribution.png")
# files.download("gc_content_distribution.png")
plt.show()

#Plot sequence length distribution and save the plot
plt.figure(figsize=(8, 6))
sns.histplot(df['Length'], kde=True)
plt.xlabel('Sequence Length')
plt.ylabel('Frequency')
plt.title('Distribution of Sequence Lengths in RNA Sequences')
# plt.savefig("sequence_length_distribution.png")
# files.download("sequence_length_distribution.png")
plt.show()

"""**Stage 5: Integration and JSON Export**"""

import json
import pandas as pd
from google.colab import files

#Loading preprocessed data
df = pd.read_csv("preprocessed_rna_data.csv")

#Convert DataFrame to dictionary format and then to JSON
output = df.to_dict(orient='records')
with open('rna_data.json', 'w') as file:
    json.dump(output, file, indent=4)

file.download('rna_data_output.json')